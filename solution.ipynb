{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e821efa1",
   "metadata": {},
   "source": [
    "# 70019 Probabilistic Inference  \n",
    "Assessed Coursework #1 (of 1)\n",
    "\n",
    "Coursework to be done either individually or in groups of two individuals\n",
    "\n",
    "## Learning goals\n",
    "\n",
    "This coursework helps you practice:\n",
    "\n",
    "• (P1) implementing Bayesian inference efficiently by exploiting model structure,  \n",
    "• (P1) understanding why this can be dramatically faster than naive marginalisation,  \n",
    "• (P2) understanding Gaussian processes as distributions over functions.  \n",
    "• (P2) implementing GP regression from scratch using NumPy.\n",
    "\n",
    "## Rules / Allowed tools\n",
    "\n",
    "• Use NumPy only for calculations (and optionally matplotlib for plotting).  \n",
    "• Do not use HMM/GP/probabilistic inference libraries (e.g., hmmlearn, pgmpy, pyro, etc.).  \n",
    "• You may write helper functions, add tests, and refactor code as you like.  \n",
    "• Please include an LLM Statement at the end noting if/how you used LLMs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9bf0e82",
   "metadata": {},
   "source": [
    "## Problem 1\n",
    "\n",
    "As an instructor, you want to understand a student's workload level over time, but you cannot observe/experience this directly (workload is a hidden state). Each day, you observe the student's engagement in your class, which is a noisy indicator of workload. Mathematically speaking, for each time step $t = 1, . . . , T$, we define the hidden state\n",
    "\n",
    "$$\n",
    "X_t \\in \\{0, 1, 2, 3, 4\\}\n",
    "$$\n",
    "\n",
    "where 0 means very low workload and 4 means extreme workload. At $t = 1, . . . , T$, you observe:\n",
    "\n",
    "$$\n",
    "y_{1:6} = [\\text{high, medium, low, absent, low, medium}].\n",
    "$$\n",
    "\n",
    "Use this provided example sequence for debugging and comparison. For timing, you may create a longer sequence of length $T = 50$ by repeating a pattern (or sampling) of engagement symbols.\n",
    "\n",
    "## Model assumptions (HMM)\n",
    "\n",
    "You assume a hidden Markov model:\n",
    "\n",
    "$$\n",
    "p(x_1) = \\pi(x_1), \\quad p(x_t \\mid x_{t-1}) = A[x_{t-1}, x_t], \\quad p(y_t \\mid x_t) = B[x_t, y_t].\n",
    "$$\n",
    "\n",
    "with hidden state size $|X| = 5$ and observation size $|Y| = 4$ as above.\n",
    "\n",
    "---\n",
    "\n",
    "## Model parameters\n",
    "\n",
    "You assume the initial distribution of workloads is roughly centred around medium:\n",
    "\n",
    "$$\n",
    "\\pi =\n",
    "\\begin{bmatrix}\n",
    "0.10 & 0.20 & 0.30 & 0.25 & 0.15\n",
    "\\end{bmatrix}.\n",
    "$$\n",
    "\n",
    "The HMM is defined with probabilities:\n",
    "\n",
    "$$\n",
    "A =\n",
    "\\begin{bmatrix}\n",
    "0.60 & 0.20 & 0.10 & 0.05 & 0.05 \\\\\n",
    "0.15 & 0.55 & 0.15 & 0.10 & 0.05 \\\\\n",
    "0.10 & 0.15 & 0.55 & 0.15 & 0.05 \\\\\n",
    "0.05 & 0.10 & 0.20 & 0.55 & 0.10 \\\\\n",
    "0.05 & 0.05 & 0.10 & 0.25 & 0.55\n",
    "\\end{bmatrix}\n",
    ";\n",
    "\\quad\n",
    "B =\n",
    "\\begin{bmatrix}\n",
    "0.05 & 0.15 & 0.30 & 0.50 \\\\\n",
    "0.05 & 0.25 & 0.45 & 0.25 \\\\\n",
    "0.10 & 0.30 & 0.40 & 0.20 \\\\\n",
    "0.20 & 0.40 & 0.25 & 0.15 \\\\\n",
    "0.35 & 0.40 & 0.15 & 0.10\n",
    "\\end{bmatrix}.\n",
    "$$\n",
    "\n",
    "Note: Rows in A correspond to workload states $0, . . . , 4$ and columns to subsequent state. Rows in B correspond to workload states $0, . . . , 4$ and columns correspond to engagement {absent, low, medium, high}."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "044a2e15",
   "metadata": {},
   "source": [
    "## What you must compute\n",
    "\n",
    "Your goal is to compute the posterior distribution over the final workload state:\n",
    "\n",
    "$$\n",
    "p(X_T \\mid y_{1:T}).\n",
    "$$\n",
    "\n",
    "a) Draw the Bayesian graphical model for $T = 4$ time steps (variables $X_1, . . . , X_4$ and $Y_1, . . . , Y_4$)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c1d3870",
   "metadata": {},
   "source": [
    "b) Write the full joint distribution $p(x_{1:T}, y_{1:T})$ for arbitrary $T$ and factorise it according to your model assumptions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1afe3d61",
   "metadata": {},
   "source": [
    "c) In the attached notebook implement a function that computes $p(X_T \\mid y_{1:T})$ by brute-force marginalisation:\n",
    "\n",
    "$$\n",
    "\\tilde{p}(x_T) := p(x_T, y_{1:T}) = \\sum_{x_{1:T-1}} p(x_{1:T}, y_{1:T});\n",
    "\\quad\n",
    "p(x_T \\mid y_{1:T}) = \\frac{\\tilde{p}(x_T)}{\\sum_{x_T} \\tilde{p}(x_T)}.\n",
    "$$\n",
    "\n",
    "This should enumerate all hidden state sequences $(x_1, . . . , x_T)$ and accumulate the probability mass onto $x_T$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08258c4d",
   "metadata": {},
   "source": [
    "d) Run your brute-force method for $T = 6$ (the provided sequence) and for $T = 8$ (extend the provided sequence however you like). Briefly describe: what is the time complexity of the brute-force method as a function of $T$ and $|X|$? Why does it scale so badly?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91bdb60d",
   "metadata": {},
   "source": [
    "e) Starting from our belief\n",
    "\n",
    "$$\n",
    "b_t(x_t) := p(x_t \\mid y_{1:t}) \\propto p(x_t, y_{1:t}),\n",
    "$$\n",
    "\n",
    "and using your factorisation from Part A, show that the beliefs satisfy the recursion\n",
    "\n",
    "$$\n",
    "b_t(x_t) \\propto p(y_t \\mid x_t)\n",
    "\\sum_{x_{t-1}} p(x_t \\mid x_{t-1}) b_{t-1}(x_{t-1}).\n",
    "$$\n",
    "\n",
    "Your derivation should clearly indicate where you used (i) the Markov property and (ii) the conditional independence of observations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "817645df",
   "metadata": {},
   "source": [
    "f) Implement a belief-update function that takes $b_{t-1}$ and returns $b_t$ using the recursion from Part C. Ensure your result is a valid probability distribution (nonnegative and sums to 1)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d146c0ab",
   "metadata": {},
   "source": [
    "g) Use your update to recursively compute $p(X_T \\mid y_{1:T})$ for the provided $T = 6$ sequence."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e16bd807",
   "metadata": {},
   "source": [
    "h) Time your efficient implementation for $T = \\{8, 50\\}$. Briefly describe: why is the filtering method so much faster than brute force?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc8ff1a7",
   "metadata": {},
   "source": [
    "## Problem 2\n",
    "\n",
    "You are given noisy observations of a 1D function:\n",
    "\n",
    "$$\n",
    "y_i = f(x_i) + \\varepsilon_i, \\quad \\varepsilon_i \\sim \\mathcal{N}(0, \\sigma^2)\n",
    "$$\n",
    "\n",
    "We place a Gaussian process prior on $f(x)$:\n",
    "\n",
    "$$\n",
    "f(x) \\sim \\text{GP}(0, k(x, x'))\n",
    "$$\n",
    "\n",
    "with RBF kernel:\n",
    "\n",
    "$$\n",
    "k(x, x') = \\sigma_f^2\n",
    "\\exp\\left(\n",
    "-\\frac{(x - x')^2}{2\\ell^2}\n",
    "\\right).\n",
    "$$\n",
    "\n",
    "a) Use the attached notebook to implement the RBF kernel, plot kernel matrices for different lengthscales (assume $\\sigma_f = 1$), and sample functions from the GP prior. Describe how lengthscale affects sample smoothness. Note that samples from a multivariate Gaussian can be taken using the Cholesky decomposition $K = LL^T$:\n",
    "\n",
    "$$\n",
    "z \\sim \\mathcal{N}(0, I)\n",
    "$$\n",
    "\n",
    "$$\n",
    "f = Lz \\sim \\mathcal{N}(0, K)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8a8146f",
   "metadata": {},
   "source": [
    "b) Use the attached notebook to implement the posterior predictive mean and covariance using Cholesky. Visually inspect the posterior samples. Describe why this method is preferred to explicitly computing a matrix inverse."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79a5e8de",
   "metadata": {},
   "source": [
    "c) Use the attached notebook to implement log marginal likelihood, perform a grid search over lengthscales, and plot log marginal likelihood vs lengthscale. What (range of) lengthscale fits best for this problem based on this search?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdc17ae0",
   "metadata": {},
   "source": [
    "d) Implement a numerical optimisation method to search for the best marginal likelihood. What lengthscale do you find is optimal?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "681c0c4a",
   "metadata": {},
   "source": [
    "e) Bonus question: it can be shown that a single-hidden-layer neural network with a particular form:\n",
    "\n",
    "$$\n",
    "f(x) = \\frac{1}{\\sqrt{m}} \\sum_{i=1}^{m} a_i \\phi(w_i^T x + b_i)\n",
    "$$\n",
    "\n",
    "converges in distribution to a GP as $m \\to \\infty$, assuming IID Gaussian weights, $a_i \\sim \\mathcal{N}(0, \\sigma_a^2)$, $w_i \\sim \\mathcal{N}(0, \\sigma_w^2 I)$, $b_i \\sim \\mathcal{N}(0, \\sigma_b^2)$. Note that this converges to some GP, whose kernel depends on the activation $\\phi$ (e.g., ReLU, tanh). Using the tanh activation for a smooth kernel function, create a sampler that samples neural networks using these assumptions for varying widths $m$. Describe what happens. At what point do you begin to see behaviour similar to the GP prior distribution?"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
